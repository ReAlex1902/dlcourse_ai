{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "Word2Vec.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBks585PRlF6",
        "colab_type": "text"
      },
      "source": [
        "# Задание 5.1 - Word2Vec\n",
        "\n",
        "В этом задании мы натренируем свои word vectors на очень небольшом датасете.\n",
        "Мы будем использовать самую простую версию word2vec, без negative sampling и других оптимизаций.\n",
        "\n",
        "Перед запуском нужно запустить скрипт `download_data.sh` чтобы скачать данные.\n",
        "\n",
        "Датасет и модель очень небольшие, поэтому это задание можно выполнить и без GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4bL31DcRlF7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# We'll use Principal Component Analysis (PCA) to visualize word vectors,\n",
        "# so make sure you install dependencies from requirements.txt!\n",
        "from sklearn.decomposition import PCA \n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5HkX4R9RlF-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fd1ab229-8d33-4545-ed7d-ba44378299b3"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRVP39xWSuYe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "45033c94-cd7b-4f63-b270-f5acdb03020f"
      },
      "source": [
        "!wget http://nlp.stanford.edu/~socherr/stanfordSentimentTreebank.zip\n",
        "!unzip stanfordSentimentTreebank.zip\n",
        "!rm stanfordSentimentTreebank.zip"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-10 17:47:19--  http://nlp.stanford.edu/~socherr/stanfordSentimentTreebank.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/~socherr/stanfordSentimentTreebank.zip [following]\n",
            "--2020-06-10 17:47:19--  https://nlp.stanford.edu/~socherr/stanfordSentimentTreebank.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6372817 (6.1M) [application/zip]\n",
            "Saving to: ‘stanfordSentimentTreebank.zip’\n",
            "\n",
            "stanfordSentimentTr 100%[===================>]   6.08M  1.94MB/s    in 3.1s    \n",
            "\n",
            "2020-06-10 17:47:23 (1.94 MB/s) - ‘stanfordSentimentTreebank.zip’ saved [6372817/6372817]\n",
            "\n",
            "Archive:  stanfordSentimentTreebank.zip\n",
            "replace stanfordSentimentTreebank/datasetSentences.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1SnFILhRlGA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "1c24b604-6483-4081-ea09-67fe4fe7b7d0"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "class StanfordTreeBank:\n",
        "    '''\n",
        "    Wrapper for accessing Stanford Tree Bank Dataset\n",
        "    https://nlp.stanford.edu/sentiment/treebank.html\n",
        "    \n",
        "    Parses dataset, gives each token and index and provides lookups\n",
        "    from string token to index and back\n",
        "    \n",
        "    Allows to generate random context with sampling strategy described in\n",
        "    word2vec paper:\n",
        "    https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        self.index_by_token = {}\n",
        "        self.token_by_index = []\n",
        "\n",
        "        self.sentences = []\n",
        "\n",
        "        self.token_freq = {}\n",
        "        \n",
        "        self.token_reject_by_index = None\n",
        "\n",
        "    def load_dataset(self, folder):\n",
        "        filename = os.path.join(folder, \"datasetSentences.txt\")\n",
        "\n",
        "        with open(filename, \"r\", encoding=\"latin1\") as f:\n",
        "            l = f.readline() # skip the first line\n",
        "            \n",
        "            for l in f:\n",
        "                splitted_line = l.strip().split()\n",
        "                words = [w.lower() for w in splitted_line[1:]] # First one is a number\n",
        "                    \n",
        "                self.sentences.append(words)\n",
        "                for word in words:\n",
        "                    if word in self.token_freq:\n",
        "                        self.token_freq[word] +=1 \n",
        "                    else:\n",
        "                        index = len(self.token_by_index)\n",
        "                        self.token_freq[word] = 1\n",
        "                        self.index_by_token[word] = index\n",
        "                        self.token_by_index.append(word)\n",
        "        self.compute_token_prob()\n",
        "                        \n",
        "    def compute_token_prob(self):\n",
        "        words_count = np.array([self.token_freq[token] for token in self.token_by_index])\n",
        "        words_freq = words_count / np.sum(words_count)\n",
        "        \n",
        "        # Following sampling strategy from word2vec paper:\n",
        "        # https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf\n",
        "        self.token_reject_by_index = 1- np.sqrt(1e-5/words_freq)\n",
        "    \n",
        "    def check_reject(self, word):\n",
        "        \"\"\"\n",
        "        Returns True if the token must NOT be rejected\n",
        "        \"\"\"\n",
        "        return np.random.rand() > self.token_reject_by_index[self.index_by_token[word]]\n",
        "        \n",
        "    def get_random_context(self, context_length=5):\n",
        "        \"\"\"\n",
        "        Returns tuple of center word and list of context words\n",
        "        \"\"\"\n",
        "        sentence_sampled = []\n",
        "        while len(sentence_sampled) <= 2:\n",
        "            sentence_index = np.random.randint(len(self.sentences)) \n",
        "            sentence = self.sentences[sentence_index]\n",
        "            sentence_sampled = [word for word in sentence if self.check_reject(word)]\n",
        "    \n",
        "        center_word_index = np.random.randint(len(sentence_sampled))\n",
        "        \n",
        "        words_before = sentence_sampled[max(center_word_index - context_length//2,0):center_word_index]\n",
        "        words_after = sentence_sampled[center_word_index+1: center_word_index+1+context_length//2]\n",
        "        \n",
        "        return sentence_sampled[center_word_index], words_before+words_after\n",
        "    \n",
        "    def num_tokens(self):\n",
        "        return len(self.token_by_index)\n",
        "        \n",
        "data = StanfordTreeBank()\n",
        "data.load_dataset(\"./stanfordSentimentTreebank/\")\n",
        "\n",
        "print(\"Num tokens:\", data.num_tokens())\n",
        "for i in range(5):\n",
        "    center_word, other_words = data.get_random_context(5)\n",
        "    print(center_word, other_words)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num tokens: 19538\n",
            "catherine ['similar', 'breillat', 'rain']\n",
            "lawrence ['comedy', 'it', 'hates', 'criticism']\n",
            "sentimentality ['autobiographical', 'brushed', 'bittersweet', 'lyric']\n",
            "transcendence ['memory', 'resistance']\n",
            "elizabethans ['little', 'crossover', 'well', 'rank']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXgVB_sTRlGD",
        "colab_type": "text"
      },
      "source": [
        "# Имплеменируем PyTorch-style Dataset для Word2Vec\n",
        "\n",
        "Этот Dataset должен сгенерировать много случайных контекстов и превратить их в сэмплы для тренировки.\n",
        "\n",
        "Напоминаем, что word2vec модель получает на вход One-hot вектор слова и тренирует простую сеть для предсказания на его основе соседних слов.\n",
        "Из набора слово-контекст создается N сэмплов (где N - количество слов в контексте):\n",
        "\n",
        "Например:\n",
        "\n",
        "Слово: `orders` и контекст: `['love', 'nicest', 'to', '50-year']` создадут 4 сэмпла:\n",
        "- input: `orders`, target: `love`\n",
        "- input: `orders`, target: `nicest`\n",
        "- input: `orders`, target: `to`\n",
        "- input: `orders`, target: `50-year`\n",
        "\n",
        "Все слова на входе и на выходе закодированы через one-hot encoding, с размером вектора равным количеству токенов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiXxoVNaRlGD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e0a9280a-4378-49da-8861-7b45d34d1f70"
      },
      "source": [
        "class Word2VecPlain(Dataset):\n",
        "    '''\n",
        "    PyTorch Dataset for plain Word2Vec. Accepts StanfordTreebank as data and is able to generate dataset based on\n",
        "    a number of random contexts.\n",
        "    '''\n",
        "    def __init__(self, data, num_contexts=30000):\n",
        "        '''\n",
        "        Initializes Word2VecPlain, but doesn't generate the samples yet (for that, use generate_dataset)\n",
        "        \n",
        "        Arguments:\n",
        "            data - StanfordTreebank instance\n",
        "            num_contexts - number of random contexts to use when generating a dataset\n",
        "        '''        \n",
        "        self.data = data\n",
        "        self.num_contexts = num_contexts\n",
        "\n",
        "        self.num_tokens = data.num_tokens()\n",
        "        self.dataset = []\n",
        "    \n",
        "    def generate_dataset(self):\n",
        "        '''\n",
        "        Generates dataset samples from random contexts\n",
        "        \n",
        "        Note: there will be more samples than contexts because every context\n",
        "        can generate more than one sample\n",
        "        '''\n",
        "        # Implement generating the dataset\n",
        "        # You should sample num_contexts contexts from the data and turn them into samples\n",
        "        # Note you will have several samples from one context\n",
        "        \n",
        "        self.dataset = []\n",
        "        \n",
        "        for _ in range(self.num_contexts):\n",
        "            word, contexts = data.get_random_context()\n",
        "            for context in contexts:\n",
        "                word_index = self.data.index_by_token[word]                \n",
        "                input_vector = torch.zeros(self.num_tokens)\n",
        "                input_vector[word_index] = 1\n",
        "                \n",
        "                context_index = self.data.index_by_token[context]\n",
        "                self.dataset.append([input_vector, context_index])\n",
        "        \n",
        "    def __len__(self):\n",
        "        '''\n",
        "        Returns total number of samples\n",
        "        '''\n",
        "        return len(self.dataset)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        '''\n",
        "        Returns i-th sample\n",
        "        \n",
        "        Return values:\n",
        "            input_vector - torch.Tensor with one-hot representation of the input vector\n",
        "            output_index - index of the target word (not torch.Tensor!)\n",
        "        '''\n",
        "        return self.dataset[index]\n",
        "\n",
        "    \n",
        "dataset = Word2VecPlain(data, 10)\n",
        "dataset.generate_dataset()\n",
        "input_vector, target = dataset[3]\n",
        "print(\"Sample - input: %s, target: %s\" % (input_vector, int(target))) # target should be able to convert to int\n",
        "\n",
        "assert isinstance(input_vector, torch.Tensor)\n",
        "assert torch.sum(input_vector) == 1.0\n",
        "assert input_vector.shape[0] == data.num_tokens()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample - input: tensor([0., 0., 0.,  ..., 0., 0., 0.]), target: 11099\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBN2rQY3RlGG",
        "colab_type": "text"
      },
      "source": [
        "# Создаем модель и тренируем ее"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJSruBikRlGG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "eeadde6a-a7a7-4387-966d-6fdb36479cbe"
      },
      "source": [
        "# Create the usual PyTorch structures\n",
        "dataset = Word2VecPlain(data, 30000)\n",
        "dataset.generate_dataset()\n",
        "\n",
        "# We'll be training very small word vectors!\n",
        "wordvec_dim = 10\n",
        "\n",
        "# We can use a standard sequential model for this\n",
        "nn_model = nn.Sequential(\n",
        "            nn.Linear(dataset.num_tokens, wordvec_dim, bias=False),\n",
        "            nn.Linear(wordvec_dim, dataset.num_tokens, bias=False), \n",
        "         )\n",
        "\n",
        "nn_model.type(torch.cuda.FloatTensor)\n",
        "nn_model.to(device)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=19538, out_features=10, bias=False)\n",
              "  (1): Linear(in_features=10, out_features=19538, bias=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZY9uEk31RlGI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_word_vectors(nn_model):\n",
        "    '''\n",
        "    Extracts word vectors from the model\n",
        "    \n",
        "    Returns:\n",
        "        input_vectors: torch.Tensor with dimensions (num_tokens, num_dimensions)\n",
        "        output_vectors: torch.Tensor with dimensions (num_tokens, num_dimensions)\n",
        "    '''\n",
        "    # Implement extracting word vectors from param weights\n",
        "    # return tuple of input vectors and output vectos \n",
        "    # Hint: you can access weights as Tensors through nn.Linear class attributes    \n",
        "    input_vectors = nn_model[0].weight.data.clone()\n",
        "    output_vectors = nn_model[1].weight.data.clone()\n",
        "    return torch.t(input_vectors), output_vectors\n",
        "\n",
        "untrained_input_vectors, untrained_output_vectors = extract_word_vectors(nn_model)\n",
        "assert untrained_input_vectors.shape == (data.num_tokens(), wordvec_dim)\n",
        "assert untrained_output_vectors.shape == (data.num_tokens(), wordvec_dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPN2F96ARlGK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(model, dataset, train_loader, optimizer, scheduler, num_epochs):\n",
        "    '''\n",
        "    Trains plain word2vec using cross-entropy loss and regenerating dataset every epoch\n",
        "    \n",
        "    Returns:\n",
        "    loss_history, train_history\n",
        "    '''\n",
        "    \n",
        "    loss = nn.CrossEntropyLoss().type(torch.FloatTensor)\n",
        "    \n",
        "    loss_history = []\n",
        "    train_history = []\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train() # Enter train mode\n",
        "        \n",
        "        dataset.generate_dataset() # Regenerate dataset every epoch\n",
        "\n",
        "        loss_accum = 0\n",
        "        correct_samples = 0\n",
        "        total_samples = 0\n",
        "        for i_step, (x, y) in enumerate(train_loader):\n",
        "          \n",
        "            x_gpu = x.to(device)\n",
        "            y_gpu = y.to(device)\n",
        "            prediction = model(x_gpu)    \n",
        "            loss_value = loss(prediction, y_gpu)\n",
        "            optimizer.zero_grad()\n",
        "            loss_value.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            _, indices = torch.max(prediction, 1)\n",
        "            correct_samples += torch.sum(indices == y_gpu)\n",
        "            total_samples += y.shape[0]\n",
        "            \n",
        "            loss_accum += loss_value\n",
        "\n",
        "        ave_loss = loss_accum / i_step\n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "        train_accuracy = float(correct_samples) / total_samples\n",
        "        \n",
        "        loss_history.append(float(ave_loss))\n",
        "        train_history.append(train_accuracy)\n",
        "        \n",
        "        # TODO Implement training for this model\n",
        "        # Note we don't have any validation set here because our purpose is the word vectors,\n",
        "        # not the predictive performance of the model\n",
        "        #\n",
        "        # And don't forget to step the learing rate scheduler!  \n",
        "        print(\"Epoch %i, Average loss: %f, Train accuracy: %f\" % (epoch, ave_loss, train_accuracy))\n",
        "        \n",
        "    return loss_history, train_history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qANkMhWRlGO",
        "colab_type": "text"
      },
      "source": [
        "# Ну и наконец тренировка!\n",
        "\n",
        "Добейтесь значения ошибки меньше **8.0**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "f4ARsrFARlGP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "outputId": "7a306eca-748a-4738-e9fa-110db5a199d7"
      },
      "source": [
        "# Finally, let's train the model!\n",
        "\n",
        "# TODO: We use placeholder values for hyperparameters - you will need to find better values!\n",
        "optimizer = optim.Adam(nn_model.parameters(), lr = 1e-3, weight_decay = 0.01)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = 100)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset, batch_size = 20)\n",
        "\n",
        "loss_history, train_history = train_model(nn_model, dataset, train_loader, optimizer, scheduler, 10)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, Average loss: 9.882490, Train accuracy: 0.000169\n",
            "Epoch 1, Average loss: 9.882471, Train accuracy: 0.000180\n",
            "Epoch 2, Average loss: 9.882499, Train accuracy: 0.000157\n",
            "Epoch 3, Average loss: 9.882491, Train accuracy: 0.000145\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-fd5d4cc3237a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mloss_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-3b4d80b25f22>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataset, train_loader, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mx_gpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0my_gpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_gpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgaF5tLfRlGR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualize training graphs\n",
        "plt.subplot(211)\n",
        "plt.plot(loss_history)\n",
        "plt.subplot(212)\n",
        "plt.plot(acc_history);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOCcQw9aRlGT",
        "colab_type": "text"
      },
      "source": [
        "# Визуализируем вектора для разного вида слов до и после тренировки\n",
        "\n",
        "В случае успешной тренировки вы должны увидеть как вектора слов разных типов (например, знаков препинания, предлогов и остальных) разделяются семантически.\n",
        "\n",
        "Студенты - в качестве выполненного задания присылайте notebook с диаграммами!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLnPnrvPRlGT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trained_input_vectors, trained_output_vectors = extract_word_vectors(nn_model)\n",
        "assert trained_input_vectors.shape == (data.num_tokens(), wordvec_dim)\n",
        "assert trained_output_vectors.shape == (data.num_tokens(), wordvec_dim)\n",
        "\n",
        "def visualize_vectors(input_vectors, output_vectors, title=''):\n",
        "    full_vectors = torch.cat((input_vectors, output_vectors), 0)\n",
        "    wordvec_embedding = PCA(n_components=2).fit_transform(full_vectors)\n",
        "\n",
        "    # Helpful words form CS244D example\n",
        "    # http://cs224d.stanford.edu/assignment1/index.html\n",
        "    visualize_words = {'green': [\"the\", \"a\", \"an\"], \n",
        "                      'blue': [\",\", \".\", \"?\", \"!\", \"``\", \"''\", \"--\"], \n",
        "                      'brown': [\"good\", \"great\", \"cool\", \"brilliant\", \"wonderful\", \n",
        "                              \"well\", \"amazing\", \"worth\", \"sweet\", \"enjoyable\"],\n",
        "                      'orange': [\"boring\", \"bad\", \"waste\", \"dumb\", \"annoying\", \"stupid\"],\n",
        "                      'red': ['tell', 'told', 'said', 'say', 'says', 'tells', 'goes', 'go', 'went']\n",
        "                     }\n",
        "\n",
        "    plt.figure(figsize=(7,7))\n",
        "    plt.suptitle(title)\n",
        "    for color, words in visualize_words.items():\n",
        "        points = np.array([wordvec_embedding[data.index_by_token[w]] for w in words])\n",
        "        for i, word in enumerate(words):\n",
        "            plt.text(points[i, 0], points[i, 1], word, color=color,horizontalalignment='center')\n",
        "        plt.scatter(points[:, 0], points[:, 1], c=color, alpha=0.3, s=0.5)\n",
        "\n",
        "visualize_vectors(untrained_input_vectors, untrained_output_vectors, \"Untrained word vectors\")\n",
        "visualize_vectors(trained_input_vectors, trained_output_vectors, \"Trained word vectors\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CM6rOW9IRlGV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}